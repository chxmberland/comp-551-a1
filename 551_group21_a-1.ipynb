{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics as skm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    1868\n",
       "1.0     409\n",
       "Name: Fitness, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Loading and cleaning dataset 1\n",
    "nhanes = pd.read_csv(\"dataset1/NHANES_age_prediction.csv\")\n",
    "\n",
    "## Rename columns\n",
    "nhanes = nhanes.rename(columns = {\n",
    "    \"SEQN\":\"Index\",\n",
    "    \"RIDAGEYR\":\"Age\",\n",
    "    \"RIAGENDR\":\"Gender\",\n",
    "    \"PAQ605\":\"Fitness\",\n",
    "    \"BMXBMI\":\"BMI\",\n",
    "    \"LBXGLU\":\"Blood_glucose\",\n",
    "    \"DIQ010\":\"Diabetic\",\n",
    "    \"LBXGLT\": \"Oral\",\n",
    "    \"LBXIN\": \"Insulin\"\n",
    "})\n",
    "\n",
    "## Checking for missing values\n",
    "# no missing values!\n",
    "nhanes.isnull().sum()\n",
    "nhanes.describe()\n",
    "\n",
    "# Wonky value of 7 for 1 row in \"Fitness\"\n",
    "nhanes[\"Fitness\"].value_counts()\n",
    "# Dropping the row:\n",
    "nhanes = nhanes.drop(nhanes[nhanes[\"Fitness\"] == 7].index)\n",
    "# Verify:\n",
    "nhanes[\"Fitness\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clump_thickness</th>\n",
       "      <th>cell_uniformity</th>\n",
       "      <th>cell_shape</th>\n",
       "      <th>marginal_adhesion</th>\n",
       "      <th>epithereal_cell_size</th>\n",
       "      <th>bare_nuclei</th>\n",
       "      <th>bland_chromatin</th>\n",
       "      <th>normal_nucleoli</th>\n",
       "      <th>mitoses</th>\n",
       "      <th>malignant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   clump_thickness  cell_uniformity  cell_shape  marginal_adhesion  \\\n",
       "0                5                4           4                  5   \n",
       "1                3                1           1                  1   \n",
       "2                6                8           8                  1   \n",
       "3                4                1           1                  3   \n",
       "4                8               10          10                  8   \n",
       "\n",
       "   epithereal_cell_size  bare_nuclei  bland_chromatin  normal_nucleoli  \\\n",
       "0                     7         10.0                3                2   \n",
       "1                     2          2.0                3                1   \n",
       "2                     3          4.0                3                7   \n",
       "3                     2          1.0                3                1   \n",
       "4                     7         10.0                9                7   \n",
       "\n",
       "   mitoses  malignant  \n",
       "0        1          0  \n",
       "1        1          0  \n",
       "2        1          0  \n",
       "3        1          0  \n",
       "4        1          1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Loading and cleaning dataset 2\n",
    "bcw = pd.read_csv(\"dataset2/breast-cancer-wisconsin.csv\")\n",
    "\n",
    "# Removing the first column as it contains ids that we don't need\n",
    "bcw = bcw.drop(bcw.columns[0], axis=1)\n",
    "\n",
    "# Creating column names\n",
    "column_names = [\"clump_thickness\",\"cell_uniformity\",\"cell_shape\",\n",
    "                \"marginal_adhesion\",\"epithereal_cell_size\",\"bare_nuclei\",\n",
    "                \"bland_chromatin\",\"normal_nucleoli\",\"mitoses\",\"class\"]\n",
    "bcw.columns = column_names\n",
    "\n",
    "# Replacing all '?' characters with NaN\n",
    "bcw.replace('?', np.nan, inplace=True)\n",
    "\n",
    "# Converting all rows to numeric values, setting any rows that can't be converted to NaN\n",
    "bcw = bcw.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Dropping all rows with NaN\n",
    "bcw = bcw.dropna()\n",
    "\n",
    "bcw[\"class\"] = [0 if x == 2 else 1 for x in bcw[\"class\"]]\n",
    "\n",
    "bcw = bcw.rename(columns = {\"class\": \"malignant\"})\n",
    "\n",
    "bcw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender proportions by age group:\n",
      "age_group  Gender\n",
      "Adult      2.0       0.512284\n",
      "           1.0       0.487716\n",
      "Senior     2.0       0.508242\n",
      "           1.0       0.491758\n",
      "Name: Gender, dtype: float64 \n",
      "\n",
      "\n",
      "Fitness levels by age group:\n",
      "age_group  Fitness\n",
      "Adult      0          0.803450\n",
      "           1          0.196550\n",
      "Senior     0          0.909341\n",
      "           1          0.090659\n",
      "Name: Fitness, dtype: float64 \n",
      "\n",
      "\n",
      "BMI summary by age group:\n",
      "             count       mean       std   min   25%   50%   75%   max\n",
      "age_group                                                           \n",
      "Adult      1913.0  27.971877  7.526883  14.5  22.6  26.8  31.4  70.1\n",
      "Senior      364.0  27.886264  5.574166  16.8  24.2  27.2  30.6  52.2 \n",
      "\n",
      "\n",
      "Blood glucose summary by age group:\n",
      "             count        mean        std   min   25%    50%    75%    max\n",
      "age_group                                                                \n",
      "Adult      1913.0   98.638787  18.258651  63.0  91.0   96.0  103.0  405.0\n",
      "Senior      364.0  104.329670  14.966670  80.0  95.0  101.0  111.0  208.0 \n",
      "\n",
      "\n",
      "Diabetic value counts by age group:\n",
      " 0    2198\n",
      "1      79\n",
      "Name: Diabetic, dtype: int64 \n",
      "\n",
      "\n",
      "age_group  Diabetic\n",
      "Adult      0           0.972295\n",
      "           1           0.027705\n",
      "Senior     0           0.928571\n",
      "           1           0.071429\n",
      "Name: Diabetic, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Adult</th>\n",
       "      <td>1913.0</td>\n",
       "      <td>12.110774</td>\n",
       "      <td>10.061061</td>\n",
       "      <td>0.14</td>\n",
       "      <td>5.9900</td>\n",
       "      <td>9.200</td>\n",
       "      <td>14.8000</td>\n",
       "      <td>102.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Senior</th>\n",
       "      <td>364.0</td>\n",
       "      <td>10.405247</td>\n",
       "      <td>7.530538</td>\n",
       "      <td>1.02</td>\n",
       "      <td>5.2475</td>\n",
       "      <td>8.465</td>\n",
       "      <td>13.2125</td>\n",
       "      <td>52.89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count       mean        std   min     25%    50%      75%     max\n",
       "age_group                                                                    \n",
       "Adult      1913.0  12.110774  10.061061  0.14  5.9900  9.200  14.8000  102.29\n",
       "Senior      364.0  10.405247   7.530538  1.02  5.2475  8.465  13.2125   52.89"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ----- Dataset 1 Summary stats ----- ###\n",
    "\n",
    "## Gender\n",
    "print(\"Gender proportions by age group:\")\n",
    "print(nhanes.groupby(\"age_group\")[\"Gender\"].value_counts(normalize = True),\n",
    "      \"\\n\\n\")\n",
    "# No apparent impact of gender upon age group\n",
    "\n",
    "## Fitness\n",
    "nhanes[\"Fitness\"] = [0 if x == 2 else 1 for x in nhanes[\"Fitness\"]]\n",
    "\n",
    "print(\"Fitness levels by age group:\")\n",
    "print(nhanes.groupby(\"age_group\")[\"Fitness\"].value_counts(normalize = True),\n",
    "      \"\\n\\n\")\n",
    "# Fitness does appear to predict/depend upon age group\n",
    "\n",
    "## BMI\n",
    "print(\"BMI summary by age group:\\n\",\n",
    "      nhanes.groupby(\"age_group\")[\"BMI\"].describe(),\n",
    "      \"\\n\\n\")\n",
    "# not crazy different, but may be signifcant\n",
    "\n",
    "## Blood Glucose\n",
    "print(\"Blood glucose summary by age group:\\n\",\n",
    "      nhanes.groupby(\"age_group\")[\"Blood_glucose\"].describe(),\n",
    "      \"\\n\\n\")\n",
    "# the seniors have noticeably higher blood glucose levels\n",
    "\n",
    "## Diabetic\n",
    "# 1: Yes diabetes\n",
    "# 2: No diabetes (to be -> 0)\n",
    "# 3: Borderline (to be -> 1)\n",
    "\n",
    "nhanes[\"Diabetic\"] = [0 if x == 2 else 1 for x in nhanes[\"Diabetic\"]]\n",
    "\n",
    "\n",
    "print(\"Diabetic value counts by age group:\\n\",\n",
    "      nhanes[\"Diabetic\"].value_counts(),\n",
    "      \"\\n\\n\")\n",
    "# Values are 2, 3, 1. 2 means not-diabetic, don't know what 1 and 3 mean\n",
    "print(nhanes.groupby(\"age_group\")[\"Diabetic\"].value_counts(normalize = True))\n",
    "# Higher proportion of 1s and 3s among seniors -- prolly big indicator\n",
    "\n",
    "## Oral\n",
    "nhanes.groupby(\"age_group\")[\"Oral\"].describe()\n",
    "# Much higher among seniors rather than adults\n",
    "\n",
    "## Insulin\n",
    "nhanes.groupby(\"age_group\")[\"Insulin\"].describe()\n",
    "# Lower among seniors vs adults\n",
    "\n",
    "\n",
    "## Variables to consider in KNN:\n",
    "#\n",
    "# - Fitness (categorical -- 2 levels)\n",
    "# - BMI (cont.)\n",
    "# - Blood glucose (cont.)\n",
    "# - Diabetic (categorical -- 3 levels)\n",
    "# - Oral (cont.)\n",
    "# - Insulin (cont.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distance functions\n",
    "\n",
    "euclidean = lambda x1, x2: np.sqrt(np.sum((x1 - x2)**2, axis=-1))\n",
    "manhattan = lambda x1, x2: np.sum(np.abs(x1 - x2), axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementation of KNN\n",
    "\n",
    "# This class will represent an instance of the KNN model with a static K\n",
    "## Distance functions:\n",
    "euclidean = lambda x1, x2: np.sqrt(np.sum((x1 - x2)**2, axis=-1))\n",
    "manhattan = lambda x1, x2: np.sum(np.abs(x1 - x2), axis=-1)\n",
    "\n",
    "## Defining the KNN class\n",
    "class KNN:\n",
    "\n",
    "    def __init__(self, K = 1, dist_fn = euclidean):\n",
    "        self.dist_fn = dist_fn\n",
    "        self.K = K\n",
    "        return\n",
    "    \n",
    "    def fit(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.C = np.max(y) + 1\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        num_test = x_test.shape[0]\n",
    "\n",
    "        distances = self.dist_fn(self.x[None,:,:], x_test[:,None,:])\n",
    "        #ith-row of knns stores the indices of k closest training samples to the ith-test sample \n",
    "        knns = np.zeros((num_test, self.K), dtype=int)\n",
    "        #ith-row of y_prob has the probability distribution over C classes\n",
    "        y_prob = np.zeros((num_test, self.C))\n",
    "        for i in range(num_test):\n",
    "            # print(i)\n",
    "            knns[i,:] = np.argsort(distances[i])[:self.K]\n",
    "            # print(knns[i,:])\n",
    "            y_prob[i,:] = np.bincount(self.y[knns[i,:]], minlength=self.C) #counts the number of instances of each class in the K-closest training samples\n",
    "        #y_prob /= np.sum(y_prob, axis=-1, keepdims=True)\n",
    "        #simply divide by K to get a probability distribution\n",
    "        y_prob /= self.K\n",
    "        return y_prob, knns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementation of DT\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, data_indices, parent):\n",
    "        self.data_indices = data_indices                    #stores the data indices which are in the region defined by this node\n",
    "        self.left = None                                    #stores the left child of the node \n",
    "        self.right = None                                   #stores the right child of the node\n",
    "        self.split_feature = None                           #the feature for split at this node\n",
    "        self.split_value = None                             #the value of the feature for split at this node\n",
    "        if parent:\n",
    "            self.depth = parent.depth + 1                   #obtain the dept of the node by adding one to dept of the parent \n",
    "            self.num_classes = parent.num_classes           #copies the num classes from the parent \n",
    "            self.data = parent.data                         #copies the data from the parent\n",
    "            self.labels = parent.labels                     #copies the labels from the parent\n",
    "            try:\n",
    "                class_prob = np.bincount(self.labels[data_indices], minlength=self.num_classes) #this is counting frequency of different labels in the region defined by this node\n",
    "            except:\n",
    "                intlabels = self.labels.astype(int)\n",
    "                class_prob = np.bincount(intlabels[data_indices], minlength=self.num_classes)\n",
    "            self.class_prob = class_prob / np.sum(class_prob)  #stores the class probability for the node\n",
    "            #note that we'll use the class probabilites of the leaf nodes for making pr\n",
    "\n",
    "\n",
    "def greedy_test(node, cost_fn):\n",
    "    #initialize the best parameter values\n",
    "    best_cost = np.inf\n",
    "    best_feature, best_value = None, None\n",
    "    num_instances, num_features = node.data.shape\n",
    "    #sort the features to get the test value candidates by taking the average of consecutive sorted feature values \n",
    "    data_sorted = np.sort(node.data[node.data_indices],axis=0)\n",
    "    test_candidates = (data_sorted[1:] + data_sorted[:-1]) / 2.\n",
    "    for f in range(num_features):\n",
    "        #stores the data corresponding to the f-th feature\n",
    "        data_f = node.data[node.data_indices, f]\n",
    "        for test in test_candidates[:,f]:\n",
    "            #Split the indices using the test value of f-th feature\n",
    "            left_indices = node.data_indices[data_f <= test]\n",
    "            right_indices = node.data_indices[data_f > test]\n",
    "            #we can't have a split where a child has zero element\n",
    "            #if this is true over all the test features and their test values  then the function returns the best cost as infinity\n",
    "            if len(left_indices) == 0 or len(right_indices) == 0:                \n",
    "                continue\n",
    "            #compute the left and right cost based on the current split                                                         \n",
    "            left_cost = cost_fn(node.labels[left_indices])\n",
    "            right_cost = cost_fn(node.labels[right_indices])\n",
    "            num_left, num_right = left_indices.shape[0], right_indices.shape[0]\n",
    "            #get the combined cost using the weighted sum of left and right cost\n",
    "            cost = (num_left * left_cost + num_right * right_cost)/num_instances\n",
    "            #update only when a lower cost is encountered\n",
    "            if cost < best_cost:\n",
    "                best_cost = cost\n",
    "                best_feature = f\n",
    "                best_value = test\n",
    "    return best_cost, best_feature, best_value\n",
    "\n",
    "\n",
    "#computes misclassification cost by subtracting the maximum probability of any class\n",
    "def cost_misclassification(labels):\n",
    "    try:\n",
    "        counts = np.bincount(labels) \n",
    "    except:\n",
    "        counts = np.bincount(labels.astype(int)) \n",
    "    class_probs = counts / np.sum(counts)\n",
    "    #you could compress both the steps above by doing class_probs = np.bincount(labels) / len(labels)\n",
    "    return 1 - np.max(class_probs)\n",
    "\n",
    "#computes entropy of the labels by computing the class probabilities\n",
    "def cost_entropy(labels):\n",
    "    try:\n",
    "        class_probs = np.bincount(labels) / len(labels)\n",
    "    except:\n",
    "        class_probs = np.bincount(labels.astype(int)) / len(labels)\n",
    "    class_probs = class_probs[class_probs > 0]              #this steps is remove 0 probabilities for removing numerical issues while computing log\n",
    "    return -np.sum(class_probs * np.log2(class_probs))       #expression for entropy -\\sigma p(x)log[p(x)]\n",
    "\n",
    "#computes the gini index cost\n",
    "def cost_gini_index(labels):\n",
    "    try:\n",
    "        class_probs = np.bincount(labels) / len(labels)\n",
    "    except:\n",
    "        class_probs = np.bincount(labels.astype(int)) / len(labels)\n",
    "    return 1 - np.sum(np.square(class_probs))               #expression for gini index 1-\\sigma p(x)^2\n",
    "\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, num_classes=None, max_depth=3, cost_fn=\"cost_entropy\", min_leaf_instances=1):\n",
    "        self.max_depth = max_depth      #maximum dept for termination \n",
    "        self.root = None                #stores the root of the decision tree \n",
    "        if cost_fn == \"cost_entropy\":\n",
    "            self.cost_fn = cost_entropy\n",
    "        elif cost_fn == \"cost_gini_index\":\n",
    "            self.cost_fn = cost_gini_index\n",
    "        elif cost_fn == \"cost_misclassification\":\n",
    "            self.cost_fn = cost_misclassification\n",
    "        else:\n",
    "            self.cost_fn = cost_entropy\n",
    "        self.num_classes = num_classes  #stores the total number of classes\n",
    "        self.min_leaf_instances = min_leaf_instances  #minimum number of instances in a leaf for termination\n",
    "        \n",
    "    def fit(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        if self.num_classes is None:\n",
    "            self.num_classes = int(np.max(labels) + 1)\n",
    "            #self.num_classes = len(np.unique(labels))\n",
    "        #below are initialization of the root of the decision tree\n",
    "        self.root = Node(np.arange(data.shape[0]), None)\n",
    "        self.root.data = data\n",
    "        self.root.labels = labels\n",
    "        self.root.num_classes = self.num_classes\n",
    "        self.root.depth = 0\n",
    "        #to recursively build the rest of the tree\n",
    "        self._fit_tree(self.root)\n",
    "        return self\n",
    "\n",
    "    def _fit_tree(self, node):\n",
    "        #This gives the condition for termination of the recursion resulting in a leaf node\n",
    "        if node.depth == self.max_depth or len(node.data_indices) <= self.min_leaf_instances:\n",
    "            return\n",
    "        #greedily select the best test by minimizing the cost\n",
    "        cost, split_feature, split_value = greedy_test(node, self.cost_fn)\n",
    "        #if the cost returned is infinity it means that it is not possible to split the node and hence terminate\n",
    "        if np.isinf(cost):\n",
    "            return\n",
    "        #print(f'best feature: {split_feature}, value {split_value}, cost {cost}')\n",
    "        #to get a boolean array suggesting which data indices corresponding to this node are in the left of the split\n",
    "        test = node.data[node.data_indices,split_feature] <= split_value\n",
    "        #store the split feature and value of the node\n",
    "        node.split_feature = split_feature\n",
    "        node.split_value = split_value\n",
    "        #define new nodes which are going to be the left and right child of the present node\n",
    "        left = Node(node.data_indices[test], node)\n",
    "        right = Node(node.data_indices[np.logical_not(test)], node)\n",
    "        #recursive call to the _fit_tree()\n",
    "        self._fit_tree(left)\n",
    "        self._fit_tree(right)\n",
    "        #assign the left and right child to present child\n",
    "        node.left = left\n",
    "        node.right = right\n",
    "    \n",
    "    def predict(self, data_test):\n",
    "        class_probs = np.zeros((data_test.shape[0], self.num_classes))\n",
    "        #class_probs = []\n",
    "        for n, x in enumerate(data_test):\n",
    "            node = self.root\n",
    "            #loop along the dept of the tree looking region where the present data sample fall in based on the split feature and value\n",
    "            while node.left:\n",
    "                if x[node.split_feature] <= node.split_value:\n",
    "                    node = node.left\n",
    "                else:\n",
    "                    node = node.right\n",
    "            #the loop terminates when you reach a leaf of the tree and the class probability of that node is taken for prediction\n",
    "            class_probs[n,:] = node.class_prob\n",
    "            #class_probs.append(node.class_prob)\n",
    "        return class_probs\n",
    "    \n",
    "    # Gets the prediction accuracy of the KNN model with an integer threshold\n",
    "    def evaluate_threshold_acc(self, probabilities: list[float], actual_labels: list[int], pos_threshold: float) -> (float, list[int]):\n",
    "        correct_predictions = 0\n",
    "        predictions = []\n",
    "\n",
    "        # Looping through each prediction and comparing it to the threshold\n",
    "        for i in range(0, len(probabilities)):\n",
    "\n",
    "            # Comparing prediction using the threshold\n",
    "            probability = probabilities[i]\n",
    "            actual = actual_labels[i]\n",
    "            label_prediction = 0 if probability < pos_threshold else 1\n",
    "\n",
    "            # Tracking the labels that are guessed\n",
    "            predictions.append(label_prediction)\n",
    "\n",
    "            # Checking to see if the prediction was correct\n",
    "            if actual == label_prediction:\n",
    "                correct_predictions += 1\n",
    "\n",
    "        # Returning the proportion of correct predictions\n",
    "        predict_acc = correct_predictions / actual_labels.size\n",
    "        return (predict_acc, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling for Dataset 1\n",
    "\n",
    "def feature_normalization(dataset):\n",
    "    for c in dataset.columns:\n",
    "        mean = dataset[c].mean()\n",
    "        st_dev = dataset[c].std()\n",
    "\n",
    "        dataset[c].apply(lambda x: (x - mean)/st_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training features array dimensions: (1143, 4)\n",
      "Training target array dimensions: (1143,) \n",
      "\n",
      "Validation features array dimensions: (564, 4)\n",
      "Validation target array dimensions: (564,) \n",
      "\n",
      "Test features array dimensions: (570, 4)\n",
      "Test target array dimensions: (570,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### KNN model 1: Dataset 1 with certain variables\n",
    "\n",
    "# Model 1: continuous variables only for simplicity\n",
    "\n",
    "nhanes[\"age_group\"] = [0 if x == \"Adult\" else 1 for x in nhanes[\"age_group\"]]\n",
    "\n",
    "nhanes_m1 = nhanes[[\"BMI\", \"Blood_glucose\", \"Oral\", \"Insulin\"]]\n",
    "nhanes_target = nhanes[\"age_group\"]\n",
    "\n",
    "\n",
    "## Step 1: splitting data into train, validation, test, roughly 50%, 25%, 25%\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    nhanes_m1, nhanes_target, test_size = 0.25, random_state = 21\n",
    "\n",
    ")\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, y_train, test_size = 0.33, random_state=22\n",
    ")\n",
    "\n",
    "print(\"Training features array dimensions:\", X_train.shape)\n",
    "print(\"Training target array dimensions:\", y_train.shape, \"\\n\")\n",
    "\n",
    "print(\"Validation features array dimensions:\", X_valid.shape)\n",
    "print(\"Validation target array dimensions:\", y_valid.shape, \"\\n\")\n",
    "\n",
    "print(\"Test features array dimensions:\", X_test.shape)\n",
    "print(\"Test target array dimensions:\", y_test.shape, \"\\n\")\n",
    "\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1143,)\n"
     ]
    },
    {
     "ename": "InvalidIndexError",
     "evalue": "(None, slice(None, None, None), slice(None, None, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/_libs/index.pyx:144\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '(None, slice(None, None, None), slice(None, None, None))' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m model_1\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_train\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 8\u001b[0m m1_preds, m1_probs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(m1_preds \u001b[38;5;241m==\u001b[39m y_test)\u001b[38;5;241m/\u001b[39my_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(accuracy)\n",
      "Cell \u001b[0;32mIn[20], line 25\u001b[0m, in \u001b[0;36mKNN.predict\u001b[0;34m(self, x_test)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_test):\n\u001b[1;32m     23\u001b[0m     num_test \u001b[38;5;241m=\u001b[39m x_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 25\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m, x_test[:,\u001b[38;5;28;01mNone\u001b[39;00m,:])\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m#ith-row of knns stores the indices of k closest training samples to the ith-test sample \u001b[39;00m\n\u001b[1;32m     27\u001b[0m     knns \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((num_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3809\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m         \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m         \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m-> 3809\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_indexing_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3810\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m \u001b[38;5;66;03m# GH#42269\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/core/indexes/base.py:5925\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5921\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m   5922\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[1;32m   5923\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[1;32m   5924\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[0;32m-> 5925\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: (None, slice(None, None, None), slice(None, None, None))"
     ]
    }
   ],
   "source": [
    "## Running KNN on dataset 1\n",
    "model_1 = KNN(K = 5)\n",
    "\n",
    "model_1.fit(X_train, y_train)\n",
    "\n",
    "print(y_train.shape)\n",
    "\n",
    "m1_preds, m1_probs = model_1.predict(X_test)\n",
    "\n",
    "accuracy = np.sum(m1_preds == y_test)/y_test.shape[0]\n",
    "\n",
    "print(accuracy)\n",
    "    \n",
    "print(m1_probs[:10])\n",
    "print(m1_preds[:10])\n",
    "print(y_test[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Section\n",
    "\n",
    "print(\"\\n----- DECISION TREE SECTION -----\\n\")\n",
    "\n",
    "cost_functions = [\"cost_misclassification\", \"cost_gini_index\", \"cost_entropy\"]\n",
    "max_max_depth = 10\n",
    "\n",
    "#util functions\n",
    "def evaluate_acc(pred_y, real_y):\n",
    "    accurate_preds = 0\n",
    "    for i in range(len(pred_y)):\n",
    "        if pred_y[i] == real_y[i]:\n",
    "            accurate_preds += 1\n",
    "\n",
    "    train_accuracy = accurate_preds / len(pred_y)\n",
    "    return train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING, PARAMETRIZING AND EVALUATION DT ON DATASET ONE\n",
    "\n",
    "print(\"\\n----- TRAINING ON DATASET ONE -----\\n\")\n",
    "\n",
    "dataset_size = nhanes.shape[0]\n",
    "num_cols = nhanes.shape[1]\n",
    "nhanes = nhanes.to_numpy()\n",
    "\n",
    "#change float col to int\n",
    "nhanes[:, 5] *= 10#.astype(int)\n",
    "nhanes[:, -1] *= 100#.astype(int)\n",
    "\n",
    "for col in range(num_cols):\n",
    "    nhanes[:, col] = (nhanes[:, col]).astype(int)\n",
    "\n",
    "inds = np.random.permutation(dataset_size)\n",
    "test_proportion = 0.25\n",
    "validate_proportion = 0.25\n",
    "test_size = int(test_proportion*dataset_size)\n",
    "validate_size = int(validate_proportion*dataset_size)\n",
    "train_size = dataset_size-test_size-validate_size\n",
    "\n",
    "\n",
    "want_to_select = [True for _ in range(num_cols)]\n",
    "#remove ID and age labels from X features\n",
    "want_to_select[0] = False\n",
    "want_to_select[1] = False\n",
    "want_to_select[2] = False\n",
    "x, y = nhanes[:,np.array(want_to_select)], nhanes[:,1]\n",
    "\n",
    "x_train, y_train = x[inds[:train_size]], y[inds[:train_size]]\n",
    "x_validate, y_validate = x[inds[train_size:train_size+validate_size]], y[inds[train_size:train_size+validate_size]]\n",
    "x_test, y_test = x[inds[train_size+validate_size:]], y[inds[train_size+validate_size:]]\n",
    "\n",
    "max_accuracy_function = None\n",
    "max_accuracy_max_depth = None\n",
    "max_accuracy = None\n",
    "\n",
    "for fn in cost_functions:\n",
    "    for max_depth in range(1,max_max_depth+1):\n",
    "        DTmodel = DecisionTree(max_depth=max_depth, cost_fn=fn)\n",
    "        DTmodel.fit(x_train, y_train)\n",
    "        train_predictedClassProbs = DTmodel.predict(x_test)\n",
    "        train_predictedClasses = []\n",
    "        for v in train_predictedClassProbs:\n",
    "            maxp = -1\n",
    "            maxIndex = -1\n",
    "            for i in range(len(v)):\n",
    "                if v[i] > maxp:\n",
    "                    maxp = v[i]\n",
    "                    maxIndex = i\n",
    "            train_predictedClasses.append(maxIndex)\n",
    "        \n",
    "        train_accurate_preds = 0\n",
    "        for i in range(len(train_predictedClasses)):\n",
    "            if train_predictedClasses[i] == y_test[i]:\n",
    "                train_accurate_preds += 1\n",
    "\n",
    "        train_accuracy = train_accurate_preds / len(train_predictedClasses)\n",
    "        #print(f'TRAIN ACCURACY ON DATASET ONE OF DECISION TREE WITH COST FUNCTION {fn} AND MAX DEPTH {max_depth} IS {train_accuracy}')\n",
    "\n",
    "        val_predictedClassProbs = DTmodel.predict(x_validate)\n",
    "        val_predictedClasses = []\n",
    "        for v in val_predictedClassProbs:\n",
    "            maxp = -1\n",
    "            maxIndex = -1\n",
    "            for i in range(len(v)):\n",
    "                if v[i] > maxp:\n",
    "                    maxp = v[i]\n",
    "                    maxIndex = i\n",
    "            val_predictedClasses.append(maxIndex)\n",
    "\n",
    "        #print(val_predictedClassProbs)\n",
    "        #print(y_validate)\n",
    "        val_accuracy = evaluate_acc(val_predictedClasses, y_validate)\n",
    "        print(f'VALIDATION ACCURACY ON DATASET ONE OF DECISION TREE WITH COST FUNCTION {fn} AND MAX DEPTH {max_depth} IS {val_accuracy}')\n",
    "\n",
    "        if max_accuracy is None or val_accuracy > max_accuracy:\n",
    "            max_accuracy_function = fn\n",
    "            max_accuracy_max_depth = max_depth\n",
    "            max_accuracy = val_accuracy\n",
    "\n",
    "print(f'BEST DECISION TREE MODEL FOR DATASET ONE HAS COST FUNCTION {max_accuracy_function} AND MAX DEPTH {max_accuracy_max_depth} WITH ACCURACY {max_accuracy}')\n",
    "\n",
    "# Testing with test data\n",
    "model = DecisionTree(max_depth=max_accuracy_max_depth, cost_fn=max_accuracy_function)\n",
    "\n",
    "# Training the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predicting labels\n",
    "probabilities = [x[1] for x in model.predict(x_test)]\n",
    "\n",
    "# Checking the prediction accuracy with a threshold of 0.5\n",
    "accuracy, _ = model.evaluate_threshold_acc(probabilities, y_test, 0.5)\n",
    "print(\"Got accuracy on test data of \" + str(round(accuracy, 2)))\n",
    "\n",
    "# Computing AUROC\n",
    "print(\"\\nGetting the AUROC score.\")\n",
    "fpr, tpr, thresholds = skm.roc_curve(y_test, probabilities)\n",
    "# Compute AUC\n",
    "auc = skm.roc_auc_score(y_test, probabilities)\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % auc)\n",
    "plt.plot([0, 1], [0, 1], color='darkgrey', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING, PARAMETRIZING AND EVALUATION DT ON DATASET TWO\n",
    "\n",
    "print(\"\\n----- TRAINING ON DATASET TWO -----\\n\")\n",
    "\n",
    "dataset_size = bcw.shape[0]\n",
    "num_cols = bcw.shape[1]\n",
    "bcw = bcw.to_numpy().astype(int)\n",
    "'''for i in range(dataset_size):\n",
    "    for j in range(num_cols):\n",
    "        if bcw[i, j] == 2:\n",
    "            bcw[i, j] = 0\n",
    "        elif bcw[i, j] == 4:\n",
    "            bcw[i,j] = 1'''\n",
    "\n",
    "inds = np.random.permutation(dataset_size)\n",
    "\n",
    "test_proportion = 0.25\n",
    "validate_proportion = 0.25\n",
    "test_size = int(test_proportion*dataset_size)\n",
    "validate_size = int(validate_proportion*dataset_size)\n",
    "train_size = dataset_size-test_size-validate_size\n",
    "\n",
    "x, y = bcw[:,:-1], bcw[:,-1]\n",
    "\n",
    "x_train, y_train = x[inds[:train_size]], y[inds[:train_size]]\n",
    "x_validate, y_validate = x[inds[train_size:train_size+validate_size]], y[inds[train_size:train_size+validate_size]]\n",
    "x_test, y_test = x[inds[train_size+validate_size:]], y[inds[train_size+validate_size:]]\n",
    "\n",
    "max_accuracy_function = None\n",
    "max_accuracy_max_depth = None\n",
    "max_accuracy = None\n",
    "for fn in cost_functions:\n",
    "    for max_depth in range(1,max_max_depth+1):\n",
    "        DTmodel = DecisionTree(max_depth=max_depth, cost_fn=fn)\n",
    "        DTmodel.fit(x_train, y_train)\n",
    "        train_predictedClassProbs = DTmodel.predict(x_test)\n",
    "        train_predictedClasses = []\n",
    "        for v in train_predictedClassProbs:\n",
    "            maxp = -1\n",
    "            maxIndex = -1\n",
    "            for i in range(len(v)):\n",
    "                if v[i] > maxp:\n",
    "                    maxp = v[i]\n",
    "                    maxIndex = i\n",
    "            train_predictedClasses.append(maxIndex)\n",
    "\n",
    "        train_accurate_preds = 0\n",
    "        for i in range(len(train_predictedClasses)):\n",
    "            if train_predictedClasses[i] == y_test[i]:\n",
    "                train_accurate_preds += 1\n",
    "\n",
    "        train_accuracy = train_accurate_preds / len(train_predictedClasses)\n",
    "        #print(f'TRAIN ACCURACY ON DATASET TWO OF DECISION TREE WITH COST FUNCTION {fn} AND MAX DEPTH {max_depth} IS {train_accuracy}')\n",
    "\n",
    "        val_predictedClassProbs = DTmodel.predict(x_validate)\n",
    "        val_predictedClasses = []\n",
    "        for v in val_predictedClassProbs:\n",
    "            maxp = -1\n",
    "            maxIndex = -1\n",
    "            for i in range(len(v)):\n",
    "                if v[i] > maxp:\n",
    "                    maxp = v[i]\n",
    "                    maxIndex = i\n",
    "            val_predictedClasses.append(maxIndex)\n",
    "\n",
    "        val_accuracy = evaluate_acc(val_predictedClasses, y_validate)\n",
    "        print(f'VALIDATION ACCURACY ON DATASET TWO OF DECISION TREE WITH COST FUNCTION {fn} AND MAX DEPTH {max_depth} IS {val_accuracy}')\n",
    "\n",
    "        if max_accuracy is None or val_accuracy > max_accuracy:\n",
    "            max_accuracy_function = fn\n",
    "            max_accuracy_max_depth = max_depth\n",
    "            max_accuracy = val_accuracy\n",
    "\n",
    "print(f'BEST DECISION TREE MODEL FOR DATASET TWO HAS COST FUNCTION {max_accuracy_function} AND MAX DEPTH {max_accuracy_max_depth} WITH ACCURACY {max_accuracy}')\n",
    "\n",
    "# Testing with test data\n",
    "model = DecisionTree(max_depth=max_accuracy_max_depth, cost_fn=max_accuracy_function)\n",
    "\n",
    "# Training the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predicting labels\n",
    "probabilities = [x[1] for x in model.predict(x_test)]\n",
    "\n",
    "# Checking the prediction accuracy with a threshold of 0.5\n",
    "accuracy, _ = model.evaluate_threshold_acc(probabilities, y_test, 0.5)\n",
    "print(\"Got accuracy on test data of \" + str(round(accuracy, 2)))\n",
    "\n",
    "# Computing AUROC\n",
    "print(\"\\nGetting the AUROC score.\")\n",
    "fpr, tpr, thresholds = skm.roc_curve(y_test, probabilities)\n",
    "\n",
    "# Compute AUC\n",
    "auc = skm.roc_auc_score(y_test, probabilities)\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % auc)\n",
    "plt.plot([0, 1], [0, 1], color='darkgrey', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic Example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
